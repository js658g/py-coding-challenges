{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color:black; font-size:180%; text-align:left;padding:3.0px; background: #ccf5ff; border-bottom: 8px solid #0047b3\" > TABLE OF CONTENTS<br><div>  \n* [IMPORTS AND INSTALLATIONS](#1)\n* [INTRODUCTION](#2)\n    * [UTILITIES](#2.1)\n    * [DATASET DETAILS](#2.2)    \n    * [CONFIGURATION](#2.3)\n    * [VERSION DETAILS](#2.4)\n* [PREPROCESSING](#3)\n* [MODEL TRAINING](#4)         ","metadata":{"id":"nUVs0Nmxd0oR"}},{"cell_type":"markdown","source":"<a id=\"1\"></a>\n# <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color:black; font-size:120%; text-align:left;padding:3.0px; background: #b3e0ff; border-bottom: 8px solid #1a0d00\" > PACKAGE IMPORTS AND INSTALLATIONS<br> <div>","metadata":{"id":"FNfLSqY-d79o"}},{"cell_type":"code","source":"%%time\n\n##################################################################\nfrom IPython.display import clear_output;\nfrom gc import collect;\nfrom google.colab import drive;\nprint();\ndrive.mount('/content/drive');\n\nfrom google.colab import files;\nfiles.upload();\n!ls -lha kaggle.json;\n!pip install -q kaggle --upgrade;\n!mkdir -p ~/.kaggle;\n!cp kaggle.json ~/.kaggle/\n!pwd;\n!chmod 600 ~/.kaggle/kaggle.json;\nprint();\n\n##################################################################\n!kaggle kernels output andreynesterov/lightautoml-038-dependencies -p /HomeCredit;\n!pip install -q /HomeCredit/lightautoml-0.3.8-py3-none-any.whl;\n\n!pip install -q --upgrade polars==0.20.5;\n!pip install -q joblib;\n!pip install -q colorama;\n!pip install -q optuna;\n!pip install -q ctypes;\n!pip install -q category_encoders;\nclear_output();\n\nimport lightgbm as lgb, sklearn as sk, pandas as pd, polars as pl, numpy as np;\nprint(f\"sklearn = {sk.__version__} | numpy = {np.__version__} | lightgbm = {lgb.__version__}\");\nprint(f\"polars = {pl.__version__} | pandas = {pd.__version__}\\n\\n\");\n\ncollect();","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"F_swKOeVdxjA","outputId":"147ec7f8-dd5d-46de-8b50-2c731b70e9dc"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":"sklearn = 1.2.2 | numpy = 1.25.2 | lightgbm = 3.2.1\n\npolars = 0.20.5 | pandas = 1.5.3\n\n\n\n\n\nCPU times: user 4.06 s, sys: 1.44 s, total: 5.5 s\n\nWall time: 4min 29s\n"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":["60"]},"metadata":{}}]},{"cell_type":"code","source":"%%time\n\n##################################################################\n# Downloading the competition data:-\n\n!kaggle competitions download -c home-credit-credit-risk-model-stability -p /HomeCredit\n!unzip /HomeCredit/home-credit-credit-risk-model-stability.zip -d /HomeCredit;\n\nimport shutil;\ntry:\n    shutil.rmtree(f\"/HomeCredit/csv_files\");\nexcept:\n    pass;\n\n##################################################################\n\nprint();\ncollect();\nclear_output();","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"TIT4wFf_dhGB","outputId":"42c4b458-e7d6-4a0c-8704-7bff5b7266bb"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":"CPU times: user 1.03 s, sys: 3.98 s, total: 5.01 s\n\nWall time: 2min 53s\n"}]},{"cell_type":"code","source":"%%time\n\nfrom gc import collect;\nfrom warnings import filterwarnings;\nfilterwarnings('ignore');\nfrom IPython.display import display_html, clear_output;\nimport ctypes;\nlibc = ctypes.CDLL(\"libc.so.6\");\n\nfrom pprint import pprint;\nfrom functools import partial;\n\nfrom copy import deepcopy;\nimport pandas as pd, numpy as np, polars as pl, os, joblib;\nimport polars.selectors as cs;\n\nfrom os import path, walk, getpid;\nfrom psutil import Process;\nimport re;\nfrom collections import Counter;\nfrom itertools import product;\nfrom glob import glob;\n\nfrom colorama import Fore, Style, init;\nfrom warnings import filterwarnings;\nfilterwarnings('ignore');\nfrom tqdm.notebook import tqdm;\n\nprint();\ncollect();","metadata":{"colab":{"background_save":true},"id":"jEzCkJZmdjzQ","outputId":"2886a166-0379-402a-86e6-7e565edf0f5c"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":"\n\nCPU times: user 101 ms, sys: 0 ns, total: 101 ms\n\nWall time: 101 ms\n"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{}}]},{"cell_type":"code","source":"%%time\n\n# Pipeline specifics:-\nfrom sklearn.model_selection import (StratifiedGroupKFold as SGKF, cross_val_score, cross_val_predict);\nfrom sklearn.pipeline import Pipeline;\nfrom sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin;\n\n# ML Model training:-\nfrom sklearn.metrics import roc_auc_score, make_scorer;\nfrom sklearn.ensemble import VotingClassifier as VC;\n\nimport torch;\nimport torch.nn as nn;\nfrom lightautoml.automl.presets.tabular_presets import TabularAutoML;\nfrom lightautoml.tasks import Task;\n\nclear_output();\n\nimport xgboost as xgb, lightgbm as lgb, catboost as cb, sklearn as sk;\nprint(f\"\\nXGBoost = {xgb.__version__} | LightGBM = {lgb.__version__} | Catboost = {cb.__version__}\");\nprint(f\"Pandas = {pd.__version__} | Sklearn = {sk.__version__}| Polars = {pl.__version__}\\n\\n\");\ncollect();","metadata":{"colab":{"background_save":true},"id":"1yoUc7f4dp0g","outputId":"a80dbe20-13ae-4fcd-ecb0-5d34cd7065f2"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":"\n\nXGBoost = 2.0.3 | LightGBM = 3.2.1 | Catboost = 1.2.2\n\nPandas = 1.5.3 | Sklearn = 1.2.2| Polars = 0.20.5\n\n\n\n\n\nCPU times: user 2.9 s, sys: 452 ms, total: 3.35 s\n\nWall time: 7.52 s\n"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":["60"]},"metadata":{}}]},{"cell_type":"code","source":"%%time\n\n# Making sklearn pipeline outputs as dataframe:-\nfrom sklearn import set_config;\nset_config(transform_output = \"pandas\");\npd.set_option('display.max_columns', 50);\npd.set_option('display.max_rows', 50);\npd.set_option('display.precision', 3);\n\n# Setting global configurations for polars:-\npl.Config.activate_decimals(True).set_tbl_hide_column_data_types(True);\npl.Config(**dict(tbl_formatting = 'ASCII_FULL_CONDENSED',\n                 tbl_hide_column_data_types = True,\n                 tbl_hide_dataframe_shape = True,\n                 fmt_float = \"mixed\",\n                 tbl_cell_alignment = 'CENTER',\n                 tbl_hide_dtype_separator = True,\n                 tbl_cols = 100,\n                 tbl_rows = 50,\n                 fmt_str_lengths = 100,\n                )\n         );","metadata":{"colab":{"background_save":true},"id":"BqljJU3Tecz0","outputId":"7130d895-976f-4fe8-b1aa-e29410b9fb10"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":"CPU times: user 841 µs, sys: 0 ns, total: 841 µs\n\nWall time: 5.25 ms\n"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":["<polars.config.Config at 0x7aaaba0b8070>"]},"metadata":{}}]},{"cell_type":"markdown","source":"<a id=\"2\"></a>\n# <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color:black; font-size:120%; text-align:left;padding:3.0px; background: #b3e0ff; border-bottom: 8px solid #1a0d00\" > INTRODUCTION<br> <div>","metadata":{"id":"QHuPn9_1ef6k"}},{"cell_type":"code","source":"%%time\n\nclass Utility:\n    \"\"\"\n    This class serves to do the below-\n    1. Define method to print in color\n    2. Define the classifier metric, custom scorer callable and competition metrics\n    3. Define the garbage cleaning process\n    \"\"\";\n\n    def PrintColor(self,text:str, color = Fore.BLUE, style = Style.BRIGHT):\n        \"Prints color outputs using colorama using a text F-string\";\n        print(style + color + text + Style.RESET_ALL);\n\n    def ScoreMetric(self, ytrue:np.array, ypred: np.array)-> float:\n        \"\"\"\n        This method calculates the classifier metric to evaluate the base-model\n        Inputs- ytrue, ypred:- np.array - input true and predictions arrays\n        Output- float:- base classifier metric, here- GINI score\n        \"\"\";\n        return roc_auc_score(ytrue, ypred);\n\n    def StabilityMetric(self, Stb_Prf: pd.DataFrame, w_fallingrate = 88, w_resstd = -0.5)-> float:\n        \"\"\"\n        This method calculates the GINI stability metric as below-\n        1. Creates an array of GINi scores from week4-91 for in-time testing\n        2. Creates a regression fit-line using numpy.polyfit\n        3. Calculates the stability measure using the formula mentioned in the competition pinned notebook\n        \"\"\";\n\n        y     = Stb_Prf.groupby(\"WEEK_NUM\").apply(lambda x: 2 * self.ScoreMetric(x[CFG.target], x[\"score\"]) -1).values;\n        x     = range(len(y));\n        a, b  = np.polyfit(x, y, 1);\n        y_hat = a * x + b;\n\n        return np.mean(y) + w_fallingrate * min(0, a) + w_resstd * np.std(y - y_hat);\n\n    def CleanMemory(self):\n        \"This method cleans the memory off unused objects and displays the cleaned state RAM usage\";\n\n        collect();\n        libc.malloc_trim(0);\n        pid        = getpid();\n        py         = Process(pid);\n        memory_use = py.memory_info()[0] / 2. ** 30;\n        return f\"\\nRAM usage = {memory_use :.4} GB\";\n\n    def PredictBatch(self, model, X: pd.DataFrame, prd_proba_req: bool = True, batch_size: int = 1000)-> np.array:\n        \"\"\"\n        This method predicts from the model in batches instead of the complete test set to avoid OOM issues in the test set\n        Inputs:-\n        1. X:- Train/ Test set\n        2. prd_proba_req:- need predict proba/ predictions- True [predict_proba], False[predict]\n        3. batch_size:- batch size to consider in one attempt\n\n        Returns:-\n        preds:- array of predicted probabilities\n        \"\"\";\n\n        num_samples = len(X);\n        num_batches = int(np.ceil(num_samples / batch_size));\n        preds       = np.zeros((num_samples,));\n\n        for batch_idx in range(num_batches):\n            self.PrintColor(f\"---> Processing batch: {batch_idx+1}/{num_batches}\", color = Fore.CYAN);\n\n            start_idx = batch_idx * batch_size;\n            end_idx   = min((batch_idx + 1) * batch_size, num_samples);\n            X_batch   = X.iloc[start_idx : end_idx];\n\n            if prd_proba_req == True:\n                batch_probs = model.predict_proba(X_batch)[:, 1];\n            else:\n                batch_probs = model.predict(X_batch).data.squeeze();\n\n            preds[start_idx: end_idx] = batch_probs;\n            _ = self.CleanMemory();\n\n        return preds;\n\nUtils = Utility();\nprint();\n","metadata":{"colab":{"background_save":true},"id":"E6nu1Zaueqdd","outputId":"59b3741b-5d5f-465b-dbb4-befc01e0e674"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":"\n\nCPU times: user 92 µs, sys: 0 ns, total: 92 µs\n\nWall time: 97.5 µs\n"}]},{"cell_type":"markdown","source":"**Data columns**<br>\nThis is available in the original data description as below<br>\nhttps://www.kaggle.com/competitions/home-credit-credit-risk-model-stability/data <br>\n<br>**Competition details and notebook objectives**<br>\n1. This is a binary classification challenge to predict home loan credit defaulters. **GINI** is the metric for the base classifier in this challenge<br>\n2. We also have to additionally assess the stability of GINI measure across time in the evaluation period. We need to score the classifier on a weekly basis and then assess the stability of the weekly GINI score using a regression model against time. Stability measure penalizes models that wane off in prediction capabilities across time. <br>\n2. In this starter notebook, we start the assignment with a simple preprocessing, understanding the data structure of the competition data, basic feature emgineering and develop starter models to initiate the challenge. We will also incorporate other opinions and approaches as we move along the challenge.<br>\n<br>\n**Model strategy** <br>\nWe start off with simple tree based ML models and Denselight model-LAMA and a soft-voting ensemble with appropriate inference in the test set submission. <br>\n<br>**References** <br>\n1. https://www.kaggle.com/code/darynarr/home-credit-drop-date-features <br>\n2. https://www.kaggle.com/code/jetakow/home-credit-2024-starter-notebook <br>\n3. https://www.kaggle.com/code/jirkaborovec/credit-risk-lgbm-optuna-hyper-params <br>\n4. https://www.kaggle.com/code/jirkaborovec/credit-risk-eda-xgboost-depth-0-1-gpu <br>\n5. https://www.kaggle.com/code/peizhengwang/lgb-xgb-cat-ensemble-baseline <br>\n6. https://www.kaggle.com/code/andreynesterov/home-credit-baseline-training-lightautoml <br>","metadata":{"id":"kfzaZAxYeudt"}},{"cell_type":"markdown","source":"<a id=\"2.3\"></a>\n## <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color:#ffffff; font-size:120%; text-align:left;padding:3.0px; background:  #008080; border-bottom: 8px solid #001a1a\" > CONFIGURATION<br><div>","metadata":{"id":"6Wd7_p60e2BS"}},{"cell_type":"code","source":"%%time\n\n# Configuration class:-\nclass CFG:\n    \"\"\"\n    Configuration class for parameters and CV strategy for tuning and training\n    \"\"\";\n\n    exp_nb             = 2;\n    version_nb         = 6;\n    test_req           = \"N\";\n    test_sample_frac   = 0.025;\n    state              = 42;\n    target             = 'target';\n    train_path         = \"/HomeCredit/parquet_files/train\";\n    test_path          = \"/HomeCredit/parquet_files/test\";\n    path               = \"/kaggle/input/home-credit-credit-risk-model-stability\";\n    model_path         = f\"/content/drive/MyDrive/HomeCreditQuality\";\n    null_cutoff        = 0.85;\n    cat_cutoff         = 200;\n    n_splits           = 3 if test_req == \"Y\" else 5;\n    n_repeats          = 1 ;\n    nbrnd_erly_stp     = 100;\n    blend_wgt          = [0.15, 0.15, 0.30, 0.20, 0.20];\n\n    # Global variables for plotting:-\n    grid_specs = {'visible': True, 'which': 'both', 'linestyle': '--',\n                  'color': 'lightgrey', 'linewidth': 0.75\n                  };\n    title_specs = {'fontsize': 9, 'fontweight': 'bold', 'color': 'tab:blue'};\n\nprint();\nUtils.PrintColor(f\"--> Configuration done!\");\n_ = Utils.PrintColor(Utils.CleanMemory());","metadata":{"colab":{"background_save":true},"id":"arG4DFTAe6By","outputId":"380a638f-52ed-497e-e6f9-85ec04d8460b"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":"\n\n\u001b[1m\u001b[34m--> Configuration done!\u001b[0m\n\n\u001b[1m\u001b[34m\n\nRAM usage = 0.8099 GB\u001b[0m\n\nCPU times: user 219 ms, sys: 0 ns, total: 219 ms\n\nWall time: 218 ms\n"}]},{"cell_type":"markdown","source":"<a id=\"2.4\"></a>\n## <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color:#ffffff; font-size:120%; text-align:left;padding:3.0px; background:  #008080; border-bottom: 8px solid #001a1a\" > VERSION DETAILS<br><div>","metadata":{"id":"NAQewn1YfIOR"}},{"cell_type":"markdown","source":"|Experiment <br> Number|Version|Details|Features| Models|CV score| Stability Score|Public LB score|\n|:-:|:-:|---|:-:|:-:|:-:|:-:|:-:|\n|2|1|* Used [public notebook](https://www.kaggle.com/code/batprem/home-credit-risk-mode-utility-scripts/notebook) features <br> * Designed LGBM XGB on feature subsets and batch predictions on multiple feature subsets <br> * Null cutoff = 50%|325|LGBM x 2 <br> XGB x 1|0.82743|0.63648||\n|2|2|* Used [public notebook](https://www.kaggle.com/code/batprem/home-credit-risk-mode-utility-scripts/notebook) features <br> * Designed LGBM XGB on feature subsets and batch predictions on multiple feature subsets <br> * Null cutoff = 70%|400|LGBM x 2 <br> XGB x 1|0.83245|0.63662||\n|2|3|* Used [public notebook](https://www.kaggle.com/code/batprem/home-credit-risk-mode-utility-scripts/notebook) features <br> * Designed LGBM on feature subsets and batch predictions on multiple feature subsets <br> * Null cutoff = 80%|422|LGBM x 2|0.83483|0.64798||\n|2|4|* Used [public notebook](https://www.kaggle.com/code/batprem/home-credit-risk-mode-utility-scripts/notebook) features <br> * Designed LGBM on feature subsets and batch predictions on multiple feature subsets <br> * Null cutoff = 95%|502|LGBM x 2|0.83615|0.65295|0.549|\n|2|5|* Used [public notebook](https://www.kaggle.com/code/kononenko/metric-s-trick-home-credit-baseline-inference) features <br> * Designed LGBM XGB combination <br> * Null cutoff > 95% <br> * Used post-processed CV score also|502|LGBM x 4 <br> XGB x 1 <br> LAMA x 1|0.83510<br>0.83836|0.65261<br>0.66045|0.61|\n|2|6|* Used [public notebook](https://www.kaggle.com/code/kononenko/metric-s-trick-home-credit-baseline-inference) features <br> * Null cutoff > 85%<br> * Used post-processed CV score also|440|LAMA x 1|0.82843|0.63397||\n\n","metadata":{"id":"0RF4jfQsfLli"}},{"cell_type":"markdown","source":"<a id=\"3\"></a>\n# <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color:black; font-size:120%; text-align:left;padding:3.0px; background: #b3e0ff; border-bottom: 8px solid #1a0d00\" > PREPROCESSING<br> <div>","metadata":{"id":"OPIsksRgfR9C"}},{"cell_type":"code","source":"%%time\n\nclass DataXformer:\n    \"\"\"\n    This is a comprehensive preprocessing and data transformer class that does the below-\n    1. consumes the input data tables\n    2. creates secondary features\n    3. ensues memory efficient outputs for the model\n    \"\"\";\n\n    def __init__(self, null_cutoff: float, cat_cutoff: int,\n                 TrainTest: str      = \"train\",\n                 sel_cols: list      = [],\n                 cat_cols: list      = [],\n                 **kwarg\n                 ):\n\n        self.TrainTest   = TrainTest;\n        self.null_cutoff = null_cutoff;\n        self.cat_cutoff  = cat_cutoff;\n        self.target      = CFG.target;\n        self.sel_cols    = sel_cols;\n        self.cat_cols    = cat_cols;\n\n        if self.TrainTest == \"train\":\n            self.path = CFG.train_path;\n        else:\n            self.path = CFG.test_path;\n\n        Utils.PrintColor(f\"\\n{'='*10} {self.TrainTest.upper()} MODE {'='*10}\\n\", color = Fore.RED);\n\n    def _TypeCastCols(self, df: pl.DataFrame):\n        \"\"\"\n        This method casts the columns into the desired dtypes with basic date handling too\n        Input- df- pl.DataFrame:- input data table\n        Output- df:- pl.DataFrame:- dataframe with type-casting\n        \"\"\";\n\n        for col in df.columns:\n            if col in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n                df = df.with_columns(pl.col(col).cast(pl.Int64));\n            elif col in [\"date_decision\"] or col[-1] in (\"D\",):\n                df = df.with_columns(pl.col(col).cast(pl.Date));\n            elif col[-1] in (\"P\", \"A\"):\n                df = df.with_columns(pl.col(col).cast(pl.Float64));\n            elif col[-1] in (\"M\",):\n                df = df.with_columns(pl.col(col).cast(pl.String));\n\n        return df;\n\n    def _MakeDtFtre(self, df: pl.DataFrame):\n        \"\"\"\n        This method creates date features from the provided dataframe\n        Input- df- pl.DataFrame:- input data table\n        Output- df- pl.DataFrame:- dataframe with date column FE\n        \"\"\";\n\n        for col in df.columns:\n            if col.endswith(\"D\"):\n                df = df.with_columns(pl.col(col) - pl.col(\"date_decision\"));\n                df = df.with_columns(pl.col(col).dt.total_days());\n                df = df.with_columns([pl.col(\"date_decision\").dt.month().alias(\"month_nb\").cast(pl.Int8),\n                                      pl.col(\"date_decision\").dt.weekday().alias(\"weekday_nb\").cast(pl.Int8),\n                                     ]\n                                    );\n        return df.drop(\"date_decision\", \"MONTH\");\n\n    def _MakeAgg(self, df: pl.DataFrame):\n        \"\"\"\n        This method makes a set of aggregate expressions for group by on case id for depth > 0 tables\n\n        Note:-\n        1. We make [max, min, first, last] aggregations for all columns\n        2. We make mean aggregation for columns ending with [P, A, D]\n        3. We make mode aggregations for columns ending with [M]\n\n        Input - df- pl.DataFrame:- input data table\n        Output- all_agg:- list of aggregate expressions to be used with group_by case_id\n        \"\"\";\n\n        all_agg = [];\n        df_cols = df.columns;\n\n        all_agg.extend([method(col).alias(f\"{method.__name__}_{col}\") \\\n                        for method in [pl.max, pl.min, pl.first, pl.last] \\\n                        for col in df_cols if col[-1] in (\"P\", \"A\", \"D\", \"M\", \"T\", \"L\") or \"num_group\" in col\n                        ]\n                       );\n        all_agg.extend([pl.col(col).mean().alias(f\"mean_{col}\") for col in df_cols if col.endswith((\"P\", \"A\", \"D\"))]);\n        all_agg.extend([pl.col(col).drop_nulls().mode().first().alias(f\"mode_{col}\") for col in df_cols if col.endswith(\"M\")]);\n        return df.group_by(\"case_id\").agg(all_agg);\n\n    def _PreProcessIpTbl(self, path:str, depth: int, isSingle: bool, **kwarg):\n        \"\"\"\n        This method does the below-\n        1. Creates chunks for file loads if we have multiple files (isSingle = False)\n        2. Concatenates the chunks to a single file with typecasting\n        3. Aggregating on case id for depth > 0 tables\n        \"\"\";\n\n        if isSingle == False:\n            components = [];\n            for path in glob(str(path)):\n                components.append(pl.scan_parquet(path).pipe(self._TypeCastCols));\n            df = pl.concat(components, how = \"vertical_relaxed\");\n        else:\n            df = pl.scan_parquet(path).pipe(self._TypeCastCols);\n\n        if depth > 0:\n            return df.pipe(self._MakeAgg);\n        else:\n            return df;\n\n    @staticmethod\n    def _ReduceMem(df: pd.DataFrame):\n        \"This method reduces memory for numeric columns in the dataframe\";\n\n        numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64', \"uint16\", \"uint32\", \"uint64\"];\n        start_mem = df.memory_usage().sum() / 1024**2;\n\n        for col in df.columns:\n            col_type = df[col].dtypes\n\n            if col_type in numerics:\n                c_min = df[col].min();\n                c_max = df[col].max();\n\n                if \"int\" in str(col_type):\n                    if c_min >= np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                        df[col] = df[col].astype(np.int8)\n                    elif c_min >= np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                        df[col] = df[col].astype(np.int16)\n                    elif c_min >= np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                        df[col] = df[col].astype(np.int32)\n                    elif c_min >= np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                        df[col] = df[col].astype(np.int64)\n                else:\n                    if c_min >= np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                        df[col] = df[col].astype(np.float16)\n                    if c_min >= np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                        df[col] = df[col].astype(np.float32)\n                    else:\n                        df[col] = df[col].astype(np.float64)\n\n        end_mem = df.memory_usage().sum() / 1024**2;\n\n        Utils.PrintColor(f\"Start - end memory:- {start_mem:5.2f} - {end_mem:5.2f} Mb\");\n        return df;\n\n    def _MakeModelData(self, df_base, depth_0, depth_1, depth_2, **kwarg):\n        \"\"\"\n        This method aggregates the input tables and joins them to make a single model table for the next steps\n        It converts the final table to a pandas dataframe for the next steps, reduces the memory consumption and selects relevant columns\n        \"\"\";\n\n        for i, df in enumerate(depth_0 + depth_1 + depth_2):\n            df_base = df_base.join(df, how = \"left\", on = \"case_id\", suffix = f\"_{i}\");\n\n        df = df_base.pipe(self._MakeDtFtre).collect().to_pandas();\n        df = self._ReduceMem(df.replace([np.inf, -1*np.inf], np.NaN));\n\n        if self.TrainTest.lower() == \"train\":\n            Utils.PrintColor(f\"---> Selecting training columns by nulls and category unique values\",\n                             color = Fore.CYAN\n                             );\n\n            drop_cols = [];\n            null_cols = df.drop(columns = [self.target], errors = \"ignore\").isna().mean();\n            drop_cols.extend(null_cols.loc[null_cols > self.null_cutoff].index.to_list());\n\n            obj_cols = df.select_dtypes(include = \"object\").columns;\n            for col in obj_cols:\n                if df[col].nunique() > self.cat_cutoff or df[col].nunique() == 1:\n                    drop_cols.append(col);\n            cat_cols  = [c for c in obj_cols if c not in drop_cols];\n\n            df = df.drop(columns = drop_cols, errors = \"ignore\");\n            df[cat_cols] = df[cat_cols].astype(\"category\");\n\n        else:\n            Utils.PrintColor(f\"---> Selecting the test-set columns and aligning category columns with train data\");\n            df                = df[self.sel_cols];\n            df[self.cat_cols] = df[self.cat_cols].astype(\"category\");\n\n        return df;\n\n    def XformData(self, display_store: bool = False):\n        \"\"\"\n        This is the cynosure method that aggregates all the inputs and prepares the FE dataset for the model training/ submission\n        \"\"\";\n\n        data_store = \\\n         {\"df_base\" : self._PreProcessIpTbl(os.path.join(self.path, f\"{self.TrainTest}_base.parquet\"), depth = 0, isSingle = True),\n\n          \"depth_0\" : [self._PreProcessIpTbl(os.path.join(self.path, f\"{self.TrainTest}_static_cb_0.parquet\"), depth = 0, isSingle = True),\n                       self._PreProcessIpTbl(os.path.join(self.path, f\"{self.TrainTest}_static_0_*.parquet\"), depth = 0, isSingle = False)\n                      ],\n\n          \"depth_1\": [self._PreProcessIpTbl(os.path.join(self.path, f\"{self.TrainTest}_applprev_1_*.parquet\"), depth = 1, isSingle = False),\n                      self._PreProcessIpTbl(os.path.join(self.path, f\"{self.TrainTest}_tax_registry_a_1.parquet\"), depth = 1, isSingle = True),\n                      self._PreProcessIpTbl(os.path.join(self.path, f\"{self.TrainTest}_tax_registry_b_1.parquet\"), depth = 1, isSingle = True),\n                      self._PreProcessIpTbl(os.path.join(self.path, f\"{self.TrainTest}_tax_registry_c_1.parquet\"), depth = 1, isSingle = True),\n                      self._PreProcessIpTbl(os.path.join(self.path, f\"{self.TrainTest}_credit_bureau_b_1.parquet\"), depth = 1, isSingle = True),\n                      self._PreProcessIpTbl(os.path.join(self.path, f\"{self.TrainTest}_other_1.parquet\"), depth = 1, isSingle = True),\n                      self._PreProcessIpTbl(os.path.join(self.path, f\"{self.TrainTest}_person_1.parquet\"), depth = 1, isSingle = True),\n                      self._PreProcessIpTbl(os.path.join(self.path, f\"{self.TrainTest}_deposit_1.parquet\"), depth = 1, isSingle = True),\n                      self._PreProcessIpTbl(os.path.join(self.path, f\"{self.TrainTest}_debitcard_1.parquet\"), depth = 1, isSingle = True)\n                      ],\n\n          \"depth_2\": [self._PreProcessIpTbl(os.path.join(self.path, f\"{self.TrainTest}_credit_bureau_b_2.parquet\"), depth = 2, isSingle = True)]\n          };\n\n        if display_store:\n            Utils.PrintColor(\"\\n---> Data store\\n\", color = Fore.CYAN);\n            pprint(data_store, width = 200, depth = 3, indent = 5);\n\n        df = self._MakeModelData(**data_store);\n        Utils.PrintColor(f\"\\n---> {self.TrainTest.capitalize()} set details = {df.shape} | {df.memory_usage().sum()/ 10**6 :,.2f} Mb\\n\",\n                         color = Fore.CYAN\n                         );\n        del data_store;\n\n        if self.TrainTest.lower() == \"train\":\n            cols = df.drop(columns = [CFG.target, \"WEEK_NUM\", \"case_id\"], errors = \"ignore\").columns;\n            joblib.dump(cols, os.path.join(CFG.model_path, f\"SelCols_E{CFG.exp_nb}V{CFG.version_nb}.pkl\"));\n            cat_cols = df.select_dtypes(include = \"category\").columns;\n            joblib.dump(cat_cols, os.path.join(CFG.model_path, f\"SelCatCols_E{CFG.exp_nb}V{CFG.version_nb}.pkl\"));\n\n            with np.printoptions(linewidth = 160):\n                Utils.PrintColor(\"\\n---> Train set columns\\n\");\n                pprint(np.array(cols));\n                Utils.PrintColor(\"\\n---> Train set category columns\\n\", color = Fore.CYAN);\n                pprint(np.array(cat_cols));\n        else:\n            pass;\n        return df;\n\nUtils.PrintColor(Utils.CleanMemory());","metadata":{"colab":{"background_save":true},"id":"2xZxKQGhAljN","outputId":"926448a3-848c-41f2-8a64-00cb9bf615ae"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":"\u001b[1m\u001b[34m\n\nRAM usage = 0.8105 GB\u001b[0m\n\nCPU times: user 209 ms, sys: 0 ns, total: 209 ms\n\nWall time: 206 ms\n"}]},{"cell_type":"code","source":"%%time\n\npp      = DataXformer(TrainTest = \"train\", null_cutoff = CFG.null_cutoff, cat_cutoff = CFG.cat_cutoff);\nXYtrain = pp.XformData(display_store = False);\nUtils.PrintColor(Utils.CleanMemory());\n\nif CFG.test_req == \"Y\":\n    XYtrain = XYtrain.groupby([\"WEEK_NUM\", CFG.target]).sample(frac = CFG.test_sample_frac).sort_index();\n    XYtrain.index = range(len(XYtrain));\n    Utils.PrintColor(f\"---> Train shape after sampling = {XYtrain.shape}\");\n\npp      = DataXformer(TrainTest   = \"test\",\n                      null_cutoff = CFG.null_cutoff,\n                      cat_cutoff  = CFG.cat_cutoff,\n                      sel_cols    = XYtrain.drop(columns = [CFG.target]).columns.tolist(),\n                      cat_cols    = XYtrain.select_dtypes(include = \"category\").columns.tolist(),\n                      );\nXtest   = pp.XformData(display_store = False);\n\nUtils.PrintColor(Utils.CleanMemory());","metadata":{"colab":{"background_save":true},"id":"UVNYRMpU00h_","outputId":"dc625f2c-90eb-4f50-ffc3-871634f3b09f"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":"\u001b[1m\u001b[31m\n\n========== TRAIN MODE ==========\n\n\u001b[0m\n\n\u001b[1m\u001b[34mStart - end memory:- 10766.64 - 6943.36 Mb\u001b[0m\n\n\u001b[1m\u001b[36m---> Selecting training columns by nulls and category unique values\u001b[0m\n\n\u001b[1m\u001b[36m\n\n---> Train set details = (1526659, 440) | 2,099.21 Mb\n\n\u001b[0m\n\n\u001b[1m\u001b[34m\n\n---> Train set columns\n\n\u001b[0m\n\narray(['birthdate_574D', 'dateofbirth_337D', 'days120_123L', 'days180_256L', 'days30_165L', 'days360_512L', 'days90_310L', 'description_5085714M',\n\n       'education_1103M', 'education_88M', 'firstquarter_103L', 'fourthquarter_440L', 'maritalst_385M', 'maritalst_893M', 'numberofqueries_373L',\n\n       'pmtscount_423L', 'pmtssum_45A', 'requesttype_4525192L', 'responsedate_1012D', 'responsedate_4527233D', 'secondquarter_766L', 'thirdquarter_1082L',\n\n       'actualdpdtolerance_344P', 'amtinstpaidbefduel24m_4187115A', 'annuity_780A', 'annuitynextmonth_57A', 'applicationcnt_361L', 'applications30d_658L',\n\n       'applicationscnt_1086L', 'applicationscnt_464L', 'applicationscnt_629L', 'applicationscnt_867L', 'avgdbddpdlast24m_3658932P',\n\n       'avgdbddpdlast3m_4187120P', 'avgdbdtollast24m_4525197P', 'avgdpdtolclosure24_3658938P', 'avginstallast24m_3658937A', 'avgmaxdpdlast9m_3716943P',\n\n       'avgoutstandbalancel6m_4187114A', 'avgpmtlast12m_4525200A', 'clientscnt12m_3712952L', 'clientscnt3m_3712950L', 'clientscnt6m_3712949L',\n\n       'clientscnt_100L', 'clientscnt_1022L', 'clientscnt_1071L', 'clientscnt_1130L', 'clientscnt_157L', 'clientscnt_257L', 'clientscnt_304L',\n\n       'clientscnt_360L', 'clientscnt_493L', 'clientscnt_533L', 'clientscnt_887L', 'clientscnt_946L', 'cntincpaycont9m_3716944L', 'cntpmts24_3658933L',\n\n       'commnoinclast6m_3546845L', 'credamount_770A', 'credtype_322L', 'currdebt_22A', 'currdebtcredtyperange_828A', 'datefirstoffer_1144D',\n\n       'datelastunpaid_3546854D', 'daysoverduetolerancedd_3976961L', 'deferredmnthsnum_166L', 'disbursedcredamount_1113A', 'disbursementtype_67L',\n\n       'downpmt_116A', 'dtlastpmtallstes_4499206D', 'eir_270L', 'firstclxcampaign_1125D', 'firstdatedue_489D', 'homephncnt_628L', 'inittransactioncode_186L',\n\n       'interestrate_311L', 'isbidproduct_1095L', 'lastactivateddate_801D', 'lastapplicationdate_877D', 'lastapprcommoditycat_1041M',\n\n       'lastapprcredamount_781A', 'lastapprdate_640D', 'lastcancelreason_561M', 'lastdelinqdate_224D', 'lastrejectcommoditycat_161M',\n\n       'lastrejectcommodtypec_5251769M', 'lastrejectcredamount_222A', 'lastrejectdate_50D', 'lastrejectreason_759M', 'lastrejectreasonclient_4145040M',\n\n       'lastst_736L', 'maininc_215A', 'mastercontrelectronic_519L', 'mastercontrexist_109L', 'maxannuity_159A', 'maxdbddpdlast1m_3658939P',\n\n       'maxdbddpdtollast12m_3658940P', 'maxdbddpdtollast6m_4187119P', 'maxdebt4_972A', 'maxdpdfrom6mto36m_3546853P', 'maxdpdinstldate_3546855D',\n\n       'maxdpdinstlnum_3546846P', 'maxdpdlast12m_727P', 'maxdpdlast24m_143P', 'maxdpdlast3m_392P', 'maxdpdlast6m_474P', 'maxdpdlast9m_1059P',\n\n       'maxdpdtolerance_374P', 'maxinstallast24m_3658928A', 'maxlnamtstart6m_4525199A', 'maxoutstandbalancel12m_4187113A', 'maxpmtlast3m_4525190A',\n\n       'mindbddpdlast24m_3658935P', 'mindbdtollast24m_4525191P', 'mobilephncnt_593L', 'monthsannuity_845L', 'numactivecreds_622L',\n\n       'numactivecredschannel_414L', 'numactiverelcontr_750L', 'numcontrs3months_479L', 'numincomingpmts_3546848L', 'numinstlallpaidearly3d_817L',\n\n       'numinstls_657L', 'numinstlsallpaid_934L', 'numinstlswithdpd10_728L', 'numinstlswithdpd5_4187116L', 'numinstlswithoutdpd_562L',\n\n       'numinstmatpaidtearly2d_4499204L', 'numinstpaid_4499208L', 'numinstpaidearly3d_3546850L', 'numinstpaidearly3dest_4493216L', 'numinstpaidearly5d_1087L',\n\n       'numinstpaidearly5dest_4493211L', 'numinstpaidearly5dobd_4499205L', 'numinstpaidearly_338L', 'numinstpaidearlyest_4493214L',\n\n       'numinstpaidlastcontr_4325080L', 'numinstpaidlate1d_3546852L', 'numinstregularpaid_973L', 'numinstregularpaidest_4493210L', 'numinsttopaygr_769L',\n\n       'numinsttopaygrest_4493213L', 'numinstunpaidmax_3546851L', 'numinstunpaidmaxest_4493212L', 'numnotactivated_1143L', 'numpmtchanneldd_318L',\n\n       'numrejects9m_859L', 'opencred_647L', 'pctinstlsallpaidearl3d_427L', 'pctinstlsallpaidlat10d_839L', 'pctinstlsallpaidlate1d_3546856L',\n\n       'pctinstlsallpaidlate4d_3546849L', 'pctinstlsallpaidlate6d_3546844L', 'pmtnum_254L', 'posfpd10lastmonth_333P', 'posfpd30lastmonth_3976960P',\n\n       'posfstqpd30lastmonth_3976962P', 'price_1097A', 'sellerplacecnt_915L', 'sellerplacescnt_216L', 'sumoutstandtotal_3546847A',\n\n       'sumoutstandtotalest_4493215A', 'totaldebt_9A', 'totalsettled_863A', 'totinstallast1m_4525188A', 'twobodfilling_608L', 'max_actualdpd_943P',\n\n       'max_annuity_853A', 'max_approvaldate_319D', 'max_byoccupationinc_3656910L', 'max_cancelreason_3545846M', 'max_childnum_21L', 'max_creationdate_885D',\n\n       'max_credacc_actualbalance_314A', 'max_credacc_credlmt_575A', 'max_credacc_maxhisbal_375A', 'max_credacc_minhisbal_90A', 'max_credacc_status_367L',\n\n       'max_credacc_transactions_402L', 'max_credamount_590A', 'max_credtype_587L', 'max_currdebt_94A', 'max_dateactivated_425D', 'max_downpmt_134A',\n\n       'max_dtlastpmt_581D', 'max_dtlastpmtallstes_3545839D', 'max_education_1138M', 'max_employedfrom_700D', 'max_familystate_726L',\n\n       'max_firstnonzeroinstldate_307D', 'max_inittransactioncode_279L', 'max_isbidproduct_390L', 'max_isdebitcard_527L', 'max_mainoccupationinc_437A',\n\n       'max_maxdpdtolerance_577P', 'max_num_group1', 'max_outstandingdebt_522A', 'max_pmtnum_8L', 'max_postype_4733339M', 'max_rejectreason_755M',\n\n       'max_rejectreasonclient_4145042M', 'max_revolvingaccount_394A', 'max_status_219L', 'max_tenor_203L', 'min_actualdpd_943P', 'min_annuity_853A',\n\n       'min_approvaldate_319D', 'min_byoccupationinc_3656910L', 'min_cancelreason_3545846M', 'min_childnum_21L', 'min_creationdate_885D',\n\n       'min_credacc_actualbalance_314A', 'min_credacc_credlmt_575A', 'min_credacc_maxhisbal_375A', 'min_credacc_minhisbal_90A', 'min_credacc_status_367L',\n\n       'min_credacc_transactions_402L', 'min_credamount_590A', 'min_credtype_587L', 'min_currdebt_94A', 'min_dateactivated_425D', 'min_downpmt_134A',\n\n       'min_dtlastpmt_581D', 'min_dtlastpmtallstes_3545839D', 'min_education_1138M', 'min_employedfrom_700D', 'min_familystate_726L',\n\n       'min_firstnonzeroinstldate_307D', 'min_inittransactioncode_279L', 'min_isdebitcard_527L', 'min_mainoccupationinc_437A', 'min_maxdpdtolerance_577P',\n\n       'min_num_group1', 'min_outstandingdebt_522A', 'min_pmtnum_8L', 'min_postype_4733339M', 'min_rejectreason_755M', 'min_rejectreasonclient_4145042M',\n\n       'min_revolvingaccount_394A', 'min_status_219L', 'min_tenor_203L', 'first_actualdpd_943P', 'first_annuity_853A', 'first_approvaldate_319D',\n\n       'first_cancelreason_3545846M', 'first_creationdate_885D', 'first_credacc_credlmt_575A', 'first_credamount_590A', 'first_credtype_587L',\n\n       'first_currdebt_94A', 'first_dateactivated_425D', 'first_downpmt_134A', 'first_dtlastpmt_581D', 'first_dtlastpmtallstes_3545839D',\n\n       'first_education_1138M', 'first_employedfrom_700D', 'first_familystate_726L', 'first_firstnonzeroinstldate_307D', 'first_inittransactioncode_279L',\n\n       'first_isbidproduct_390L', 'first_mainoccupationinc_437A', 'first_maxdpdtolerance_577P', 'first_num_group1', 'first_outstandingdebt_522A',\n\n       'first_pmtnum_8L', 'first_postype_4733339M', 'first_rejectreason_755M', 'first_rejectreasonclient_4145042M', 'first_status_219L', 'first_tenor_203L',\n\n       'last_actualdpd_943P', 'last_annuity_853A', 'last_approvaldate_319D', 'last_byoccupationinc_3656910L', 'last_cancelreason_3545846M',\n\n       'last_childnum_21L', 'last_creationdate_885D', 'last_credacc_credlmt_575A', 'last_credamount_590A', 'last_credtype_587L', 'last_currdebt_94A',\n\n       'last_dateactivated_425D', 'last_downpmt_134A', 'last_dtlastpmt_581D', 'last_dtlastpmtallstes_3545839D', 'last_education_1138M',\n\n       'last_employedfrom_700D', 'last_familystate_726L', 'last_firstnonzeroinstldate_307D', 'last_inittransactioncode_279L', 'last_isbidproduct_390L',\n\n       'last_mainoccupationinc_437A', 'last_maxdpdtolerance_577P', 'last_num_group1', 'last_outstandingdebt_522A', 'last_pmtnum_8L', 'last_postype_4733339M',\n\n       'last_rejectreason_755M', 'last_rejectreasonclient_4145042M', 'last_status_219L', 'last_tenor_203L', 'mean_actualdpd_943P', 'mean_annuity_853A',\n\n       'mean_approvaldate_319D', 'mean_creationdate_885D', 'mean_credacc_actualbalance_314A', 'mean_credacc_credlmt_575A', 'mean_credacc_maxhisbal_375A',\n\n       'mean_credacc_minhisbal_90A', 'mean_credamount_590A', 'mean_currdebt_94A', 'mean_dateactivated_425D', 'mean_downpmt_134A', 'mean_dtlastpmt_581D',\n\n       'mean_dtlastpmtallstes_3545839D', 'mean_employedfrom_700D', 'mean_firstnonzeroinstldate_307D', 'mean_mainoccupationinc_437A',\n\n       'mean_maxdpdtolerance_577P', 'mean_outstandingdebt_522A', 'mean_revolvingaccount_394A', 'mode_cancelreason_3545846M', 'mode_education_1138M',\n\n       'mode_postype_4733339M', 'mode_rejectreason_755M', 'mode_rejectreasonclient_4145042M', 'max_amount_4527230A', 'max_num_group1_3',\n\n       'max_recorddate_4527225D', 'min_amount_4527230A', 'min_num_group1_3', 'min_recorddate_4527225D', 'first_amount_4527230A', 'first_num_group1_3',\n\n       'first_recorddate_4527225D', 'last_amount_4527230A', 'last_num_group1_3', 'last_recorddate_4527225D', 'mean_amount_4527230A',\n\n       'mean_recorddate_4527225D', 'max_num_group1_5', 'max_pmtamount_36A', 'max_processingdate_168D', 'min_num_group1_5', 'min_pmtamount_36A',\n\n       'min_processingdate_168D', 'first_num_group1_5', 'first_pmtamount_36A', 'first_processingdate_168D', 'last_num_group1_5', 'last_pmtamount_36A',\n\n       'last_processingdate_168D', 'mean_pmtamount_36A', 'mean_processingdate_168D', 'max_birth_259D', 'max_contaddr_smempladdr_334L', 'max_education_927M',\n\n       'max_empl_employedfrom_271D', 'max_empl_employedtotal_800L', 'max_empl_industry_691L', 'max_empladdr_district_926M', 'max_empladdr_zipcode_114M',\n\n       'max_familystate_447L', 'max_incometype_1044T', 'max_language1_981M', 'max_mainoccupationinc_384A', 'max_num_group1_8', 'max_personindex_1023L',\n\n       'max_persontype_1072L', 'max_persontype_792L', 'max_relationshiptoclient_415T', 'max_relationshiptoclient_642T', 'max_role_1084L',\n\n       'max_safeguarantyflag_411L', 'max_sex_738L', 'max_type_25L', 'min_birth_259D', 'min_contaddr_smempladdr_334L', 'min_education_927M',\n\n       'min_empl_employedfrom_271D', 'min_empl_employedtotal_800L', 'min_empl_industry_691L', 'min_familystate_447L', 'min_incometype_1044T',\n\n       'min_language1_981M', 'min_mainoccupationinc_384A', 'min_num_group1_8', 'min_personindex_1023L', 'min_persontype_1072L', 'min_persontype_792L',\n\n       'min_relationshiptoclient_415T', 'min_relationshiptoclient_642T', 'min_safeguarantyflag_411L', 'min_sex_738L', 'min_type_25L', 'first_birth_259D',\n\n       'first_contaddr_smempladdr_334L', 'first_education_927M', 'first_empl_employedfrom_271D', 'first_empl_employedtotal_800L', 'first_empl_industry_691L',\n\n       'first_familystate_447L', 'first_incometype_1044T', 'first_language1_981M', 'first_mainoccupationinc_384A', 'first_num_group1_8',\n\n       'first_personindex_1023L', 'first_persontype_1072L', 'first_persontype_792L', 'first_role_1084L', 'first_safeguarantyflag_411L', 'first_sex_738L',\n\n       'first_type_25L', 'last_birth_259D', 'last_contaddr_smempladdr_334L', 'last_education_927M', 'last_empladdr_district_926M',\n\n       'last_empladdr_zipcode_114M', 'last_incometype_1044T', 'last_language1_981M', 'last_mainoccupationinc_384A', 'last_num_group1_8',\n\n       'last_personindex_1023L', 'last_persontype_1072L', 'last_persontype_792L', 'last_relationshiptoclient_642T', 'last_role_1084L',\n\n       'last_safeguarantyflag_411L', 'last_sex_738L', 'last_type_25L', 'mean_birth_259D', 'mean_empl_employedfrom_271D', 'mean_mainoccupationinc_384A',\n\n       'mode_education_927M', 'mode_language1_981M', 'month_nb', 'weekday_nb'], dtype=object)\n\n\u001b[1m\u001b[36m\n\n---> Train set category columns\n\n\u001b[0m\n\narray(['description_5085714M', 'education_1103M', 'education_88M', 'maritalst_385M', 'maritalst_893M', 'requesttype_4525192L', 'credtype_322L',\n\n       'disbursementtype_67L', 'inittransactioncode_186L', 'lastapprcommoditycat_1041M', 'lastcancelreason_561M', 'lastrejectcommoditycat_161M',\n\n       'lastrejectcommodtypec_5251769M', 'lastrejectreason_759M', 'lastrejectreasonclient_4145040M', 'lastst_736L', 'opencred_647L', 'twobodfilling_608L',\n\n       'max_cancelreason_3545846M', 'max_credacc_status_367L', 'max_credtype_587L', 'max_education_1138M', 'max_familystate_726L',\n\n       'max_inittransactioncode_279L', 'max_isbidproduct_390L', 'max_isdebitcard_527L', 'max_postype_4733339M', 'max_rejectreason_755M',\n\n       'max_rejectreasonclient_4145042M', 'max_status_219L', 'min_cancelreason_3545846M', 'min_credacc_status_367L', 'min_credtype_587L',\n\n       'min_education_1138M', 'min_familystate_726L', 'min_inittransactioncode_279L', 'min_isdebitcard_527L', 'min_postype_4733339M', 'min_rejectreason_755M',\n\n       'min_rejectreasonclient_4145042M', 'min_status_219L', 'first_cancelreason_3545846M', 'first_credtype_587L', 'first_education_1138M',\n\n       'first_familystate_726L', 'first_inittransactioncode_279L', 'first_isbidproduct_390L', 'first_postype_4733339M', 'first_rejectreason_755M',\n\n       'first_rejectreasonclient_4145042M', 'first_status_219L', 'last_cancelreason_3545846M', 'last_credtype_587L', 'last_education_1138M',\n\n       'last_familystate_726L', 'last_inittransactioncode_279L', 'last_isbidproduct_390L', 'last_postype_4733339M', 'last_rejectreason_755M',\n\n       'last_rejectreasonclient_4145042M', 'last_status_219L', 'mode_cancelreason_3545846M', 'mode_education_1138M', 'mode_postype_4733339M',\n\n       'mode_rejectreason_755M', 'mode_rejectreasonclient_4145042M', 'max_contaddr_smempladdr_334L', 'max_education_927M', 'max_empl_employedtotal_800L',\n\n       'max_empl_industry_691L', 'max_empladdr_district_926M', 'max_empladdr_zipcode_114M', 'max_familystate_447L', 'max_incometype_1044T',\n\n       'max_language1_981M', 'max_relationshiptoclient_415T', 'max_relationshiptoclient_642T', 'max_role_1084L', 'max_safeguarantyflag_411L', 'max_sex_738L',\n\n       'max_type_25L', 'min_contaddr_smempladdr_334L', 'min_education_927M', 'min_empl_employedtotal_800L', 'min_empl_industry_691L', 'min_familystate_447L',\n\n       'min_incometype_1044T', 'min_language1_981M', 'min_relationshiptoclient_415T', 'min_relationshiptoclient_642T', 'min_safeguarantyflag_411L',\n\n       'min_sex_738L', 'min_type_25L', 'first_contaddr_smempladdr_334L', 'first_education_927M', 'first_empl_employedtotal_800L', 'first_empl_industry_691L',\n\n       'first_familystate_447L', 'first_incometype_1044T', 'first_language1_981M', 'first_role_1084L', 'first_safeguarantyflag_411L', 'first_sex_738L',\n\n       'first_type_25L', 'last_contaddr_smempladdr_334L', 'last_education_927M', 'last_empladdr_district_926M', 'last_empladdr_zipcode_114M',\n\n       'last_incometype_1044T', 'last_language1_981M', 'last_relationshiptoclient_642T', 'last_role_1084L', 'last_safeguarantyflag_411L', 'last_sex_738L',\n\n       'last_type_25L', 'mode_education_927M', 'mode_language1_981M'], dtype=object)\n\n\u001b[1m\u001b[34m\n\nRAM usage = 3.669 GB\u001b[0m\n\n\u001b[1m\u001b[31m\n\n========== TEST MODE ==========\n\n\u001b[0m\n\n\u001b[1m\u001b[34mStart - end memory:-  0.07 -  0.06 Mb\u001b[0m\n\n\u001b[1m\u001b[34m---> Selecting the test-set columns and aligning category columns with train data\u001b[0m\n\n\u001b[1m\u001b[36m\n\n---> Test set details = (10, 439) | 0.03 Mb\n\n\u001b[0m\n\n\u001b[1m\u001b[34m\n\nRAM usage = 3.148 GB\u001b[0m\n\nCPU times: user 16min 20s, sys: 9min 6s, total: 25min 27s\n\nWall time: 20min 47s\n"}]},{"cell_type":"markdown","source":"<a id=\"4\"></a>\n# <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color:black; font-size:120%; text-align:left;padding:3.0px; background: #b3e0ff; border-bottom: 8px solid #1a0d00\" > MODEL TRAINING<br> <div>","metadata":{"id":"-htSH8n28ORa"}},{"cell_type":"code","source":"%%time\n\nmodel = TabularAutoML(task = Task('binary', loss = 'logloss', metric = 'auc'),\n                      timeout   = 10000,\n                      cpu_limit = 4,\n                      gpu_ids   = '0',\n                      general_params = {\"use_algos\": [[\"denselight\"]]},\n                      nn_params = {\"n_epochs\"       : 5,\n                                   \"bs\"             : 128,\n                                   \"num_workers\"    : 0,\n                                   \"path_to_save\"   : None,\n                                   \"freeze_defaults\": True,\n                                   \"cont_embedder\"  : \"cont\",\n                                   },\n                      nn_pipeline_params = {\"use_qnt\": False, \"use_te\": False},\n                      reader_params = {'n_jobs': 4,\n                                       'cv'    : CFG.n_splits,\n                                       'random_state': CFG.state,\n                                       'advanced_roles': False\n                                       },\n                      );\n\noof_preds = \\\nmodel.fit_predict(XYtrain,\n                  roles   = {'target': CFG.target,'group': \"WEEK_NUM\",'drop' : ['case_id', \"WEEK_NUM\"]},\n                  verbose = 0\n                  );\n\n","metadata":{"colab":{"background_save":true},"id":"PvSpEfVbfOFm","outputId":"ac8fda03-94aa-4806-d036-f261b2bf5d5f"},"execution_count":null,"outputs":[{"name":"stderr","output_type":"stream","text":"INFO:lightautoml.automl.presets.base:Stdout logging level is ERROR.\n\nWARNING:lightautoml.utils.timer:Copying TaskTimer may affect the parent PipelineTimer, so copy will create new unlimited TaskTimer\n\nINFO:lightautoml.automl.presets.base:Task: binary\n\n\n\nINFO:lightautoml.automl.presets.base:Start automl preset with listed constraints:\n\nINFO:lightautoml.automl.presets.base:- time: 10000.00 seconds\n\nINFO:lightautoml.automl.presets.base:- CPU: 4 cores\n\nINFO:lightautoml.automl.presets.base:- memory: 16 GB\n\n\n\nINFO:lightautoml.reader.base:\u001b[1mTrain data shape: (1526659, 440)\u001b[0m\n\n\n\nINFO:lightautoml.automl.base:Layer \u001b[1m1\u001b[0m train process start. Time left 9995.28 secs\n\nDEBUG:lightautoml.ml_algo.dl_model:number of text features: 0 \n\nDEBUG:lightautoml.ml_algo.dl_model:number of categorical features: 99 \n\nDEBUG:lightautoml.ml_algo.dl_model:number of continuous features: 324 \n\nINFO:lightautoml.ml_algo.base:Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_denselight_0\u001b[0m ...\n\nDEBUG:lightautoml.ml_algo.base:Training params: {'num_workers': 0, 'pin_memory': False, 'max_length': 256, 'is_snap': False, 'input_bn': False, 'max_emb_size': 256, 'bert_name': None, 'pooling': 'cls', 'device': device(type='cuda', index=0), 'use_cont': True, 'use_cat': True, 'use_text': False, 'lang': 'en', 'deterministic': True, 'multigpu': False, 'random_state': 42, 'model': 'denselight', 'model_with_emb': False, 'path_to_save': None, 'verbose_inside': None, 'verbose': 1, 'n_epochs': 5, 'snap_params': {'k': 3, 'early_stopping': True, 'patience': 10, 'swa': True}, 'bs': 128, 'emb_dropout': 0.1, 'emb_ratio': 3, 'opt': 'Adam', 'opt_params': {'lr': 0.0003, 'weight_decay': 0}, 'sch': 'ReduceLROnPlateau', 'scheduler_params': {'patience': 5, 'factor': 0.5, 'min_lr': 1e-05}, 'loss': None, 'loss_params': {}, 'loss_on_logits': True, 'clip_grad': False, 'clip_grad_params': {}, 'init_bias': True, 'dataset': 'UniversalDataset', 'tuned': False, 'optimization_search_space': None, 'verbose_bar': False, 'freeze_defaults': True, 'n_out': 1, 'hid_factor': [2, 2], 'hidden_size': [512, 256], 'block_config': [2, 2], 'compression': 0.5, 'growth_size': 256, 'bn_factor': 2, 'drop_rate': 0.1, 'noise_std': 0.05, 'num_init_features': None, 'act_fun': 'LeakyReLU', 'use_noise': False, 'use_bn': True, 'embedding_size': 10, 'cat_embedder': 'cat', 'cont_embedder': 'cont', 'stop_by_metric': False, 'tuning_params': {'fit_on_holdout': True, 'max_tuning_iter': 25, 'max_tuning_time': 3600}, 'device_ids': None, 'num_dims': 324, 'text_features': [], 'bias': array([-3.42781893])}\n\nINFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_denselight_0\u001b[0m =====\n\nINFO3:lightautoml.text.trainer:Epoch: 0, train loss: 0.11984385550022125, val loss: 0.1178249940276146, val metric: 0.8204359571834102\n\nINFO3:lightautoml.text.trainer:Epoch: 1, train loss: 0.11724332720041275, val loss: 0.1169477105140686, val metric: 0.8195744582152832\n\nINFO3:lightautoml.text.trainer:Epoch: 2, train loss: 0.11620213836431503, val loss: 0.11594848334789276, val metric: 0.8257879578208112\n\nINFO3:lightautoml.text.trainer:Epoch: 3, train loss: 0.11534249037504196, val loss: 0.11578461527824402, val metric: 0.8269336190217591\n\nINFO3:lightautoml.text.trainer:Epoch: 4, train loss: 0.11468042433261871, val loss: 0.11564755439758301, val metric: 0.828899178883792\n\nINFO3:lightautoml.text.trainer:Early stopping: val loss: 0.11524888128042221, val metric: 0.8298646443613802\n\nINFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_denselight_0\u001b[0m =====\n\nINFO3:lightautoml.text.trainer:Epoch: 0, train loss: 0.11971433460712433, val loss: 0.11808427423238754, val metric: 0.815798298379991\n\nINFO3:lightautoml.text.trainer:Epoch: 1, train loss: 0.11704428493976593, val loss: 0.11658959090709686, val metric: 0.8211122745912635\n\nINFO3:lightautoml.text.trainer:Epoch: 2, train loss: 0.1160917803645134, val loss: 0.11614881455898285, val metric: 0.8247183336711139\n\nINFO3:lightautoml.text.trainer:Epoch: 3, train loss: 0.11522186547517776, val loss: 0.1164582222700119, val metric: 0.8244418235979958\n\nINFO3:lightautoml.text.trainer:Epoch: 4, train loss: 0.11462264508008957, val loss: 0.11629192531108856, val metric: 0.8245915163631936\n\nINFO3:lightautoml.text.trainer:Early stopping: val loss: 0.11554970592260361, val metric: 0.8271254567016132\n\nINFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_denselight_0\u001b[0m =====\n\nINFO3:lightautoml.text.trainer:Epoch: 0, train loss: 0.11983772367238998, val loss: 0.11727283895015717, val metric: 0.8197192149896072\n\nINFO3:lightautoml.text.trainer:Epoch: 1, train loss: 0.11729413270950317, val loss: 0.11607446521520615, val metric: 0.8243484510452534\n\nINFO3:lightautoml.text.trainer:Epoch: 2, train loss: 0.11617223918437958, val loss: 0.11578299105167389, val metric: 0.826417527921718\n\nINFO3:lightautoml.text.trainer:Epoch: 3, train loss: 0.11534347385168076, val loss: 0.11610984057188034, val metric: 0.8275468531839858\n\nINFO3:lightautoml.text.trainer:Epoch: 4, train loss: 0.11460983753204346, val loss: 0.11553294956684113, val metric: 0.8271798256970087\n\nINFO3:lightautoml.text.trainer:Early stopping: val loss: 0.1153537705540657, val metric: 0.8282363017648204\n\nINFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 3\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_denselight_0\u001b[0m =====\n\nINFO3:lightautoml.text.trainer:Epoch: 0, train loss: 0.11982983350753784, val loss: 0.11810797452926636, val metric: 0.8183981064730199\n\nINFO3:lightautoml.text.trainer:Epoch: 1, train loss: 0.11715342104434967, val loss: 0.11669784039258957, val metric: 0.8222644430782104\n\nINFO3:lightautoml.text.trainer:Epoch: 2, train loss: 0.11622951179742813, val loss: 0.11591481417417526, val metric: 0.825354815462639\n\nINFO3:lightautoml.text.trainer:Epoch: 3, train loss: 0.11531637609004974, val loss: 0.1160433366894722, val metric: 0.8255939885622545\n\nINFO3:lightautoml.text.trainer:Epoch: 4, train loss: 0.11465979367494583, val loss: 0.11548671126365662, val metric: 0.8277959606169321\n\nINFO3:lightautoml.text.trainer:Early stopping: val loss: 0.11534590274095535, val metric: 0.828441194879857\n\nINFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 4\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_denselight_0\u001b[0m =====\n\nINFO3:lightautoml.text.trainer:Epoch: 0, train loss: 0.11972785741090775, val loss: 0.11752526462078094, val metric: 0.8168038095462578\n\nINFO3:lightautoml.text.trainer:Epoch: 1, train loss: 0.1172279417514801, val loss: 0.11695149540901184, val metric: 0.8215018411906467\n\nINFO3:lightautoml.text.trainer:Epoch: 2, train loss: 0.11614903062582016, val loss: 0.11591020226478577, val metric: 0.8251167235620258\n\nINFO3:lightautoml.text.trainer:Epoch: 3, train loss: 0.11531000584363937, val loss: 0.11571557074785233, val metric: 0.8262137786867803\n\nINFO3:lightautoml.text.trainer:Epoch: 4, train loss: 0.1146889328956604, val loss: 0.1155429407954216, val metric: 0.8272918297464029\n\nINFO3:lightautoml.text.trainer:Early stopping: val loss: 0.11521251499652863, val metric: 0.8288449206046961\n\nINFO:lightautoml.ml_algo.base:Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_denselight_0\u001b[0m finished. score = \u001b[1m0.8284312001566657\u001b[0m\n\nINFO:lightautoml.ml_algo.base:\u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_denselight_0\u001b[0m fitting and predicting completed\n\nINFO:lightautoml.automl.base:Time left 5357.18 secs\n\n\n\nINFO:lightautoml.automl.base:\u001b[1mLayer 1 training completed.\u001b[0m\n\n\n\nINFO:lightautoml.automl.presets.base:\u001b[1mAutoml preset training completed in 4643.21 seconds\u001b[0m\n\n\n\nINFO:lightautoml.automl.presets.base:Model description:\n\nFinal prediction for new objects (level 0) = \n\n\t 1.00000 * (5 averaged models Lvl_0_Pipe_0_Mod_0_TorchNN_denselight_0) \n\n\n"},{"name":"stdout","output_type":"stream","text":"CPU times: user 4h 47min 3s, sys: 1min, total: 4h 48min 3s\n\nWall time: 1h 17min 23s\n"}]},{"cell_type":"code","source":"%%time\n\nUtils.PrintColor(f\"OOF score = {Utils.ScoreMetric(XYtrain[CFG.target], oof_preds.data):.5f}\");\nstb_metric = Utils.StabilityMetric(XYtrain[[\"WEEK_NUM\", CFG.target]].assign(score = oof_preds.data.flatten()));\nUtils.PrintColor(f\"Stability Metric = {stb_metric:.5f}\\n\\n\", color = Fore.CYAN);\n\njoblib.dump(model, os.path.join(CFG.model_path, f\"E{CFG.exp_nb}V{CFG.version_nb}_DENSELIGHT.model\"));\n\n_ = Utils.CleanMemory();","metadata":{"colab":{"background_save":true},"id":"LPb9bRbbxPKf","outputId":"8d9f26cb-31f8-4040-a764-12b54e97b831"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":"\u001b[1m\u001b[34mOOF score = 0.82843\u001b[0m\n\n\u001b[1m\u001b[36mStability Metric = 0.63397\n\n\n\n\u001b[0m\n\nCPU times: user 1.6 s, sys: 32 ms, total: 1.63 s\n\nWall time: 1.74 s\n"}]}]}